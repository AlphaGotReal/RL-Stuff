#! /usr/bin/python3

import os
from random import random
from math import sin, cos, tanh, atan2, sqrt, pi

import rospy
from geometry_msgs.msg import Point, Pose, Twist, Vector3
from nav_msgs.msg import Odometry
from visualization_msgs.msg import Marker, MarkerArray
from std_srvs.srv import Empty

from skeleton import Agent

use_weights = False if os.sys.argv[1][0].lower() == 'n' else f"{os.sys.argv[1]}.pth"
store_weights = f"{os.sys.argv[2]}.pth"

class TrainerNode():

    def __init__(self):

        rospy.init_node("trainer")

        # agent init

        self.agent = Agent(
            observation_dims=4,
            n_actions=2,
            alpha=0.001,
            gamma=0.9,
            reuse=use_weights
        )

        self.distance:float = None
        self.steer:float = None

        self.goal_tolerance = 0.6

        self.odom:Pose = None
        self.goal:Pose = None

        self.vel:Twist = Twist()

        self.episode = 0
        self.epochs = 0
        self.episode_return = 0

        self.observations = []
        self.rewards = []

        # ros init

        rospy.Subscriber("/odom", Odometry, self.get_odom)
        
        self.vel_pub = rospy.Publisher("/cmd_vel", Twist, queue_size=1)
        self.goal_pub = rospy.Publisher("/goal", Marker, queue_size=1)
        self.confidence_pub = rospy.Publisher("/confidence", Marker, queue_size=1)

        self.goal_marker = Marker()
        self.goal_marker.header.frame_id = "odom"
        self.goal_marker.ns = "goal"
        self.goal_marker.id = 0
        self.goal_marker.type = Marker.SPHERE
        self.goal_marker.action = Marker.ADD
        self.goal_marker.color.r = 0
        self.goal_marker.color.g = 0
        self.goal_marker.color.b = 1
        self.goal_marker.color.a = 1
        self.goal_marker.scale = Vector3(self.goal_tolerance, self.goal_tolerance, self.goal_tolerance)

        self.confidence_bar = Marker()
        self.confidence_bar.header.frame_id = "odom"
        self.confidence_bar.ns = "confidence"
        self.confidence_bar.id = 1
        self.confidence_bar.type = Marker.CYLINDER
        self.confidence_bar.action = Marker.ADD
        self.confidence_bar.color.r = 1
        self.confidence_bar.color.g = 0
        self.confidence_bar.color.b = 0
        self.confidence_bar.color.a = 1
        self.confidence_bar.scale = Vector3(0.2, 0.2, 0)
        self.confidence_bar.pose.position = Point(0, 0, 3)

    def get_odom(self, odom):
        self.odom = odom.pose.pose

    def respawn(self):
        try:
            reset = rospy.ServiceProxy("/gazebo/reset_simulation", Empty)
            reset()
        except rospy.ServiceException as e:
            print(f"{type(e): {e}}")

    def generate_random_goal(self, distance):
        if (self.odom is None):
            return None

        theta = random() * 2*pi
        self.goal = Pose()
        self.goal.position = Point(distance*cos(theta), distance*sin(theta), 0)
        self.goal_marker.pose = self.goal

    def calculate_current_state(self):
        if (self.goal is None or self.odom is None):
            return None

        dx = self.goal.position.x - self.odom.position.x
        dy = self.goal.position.y - self.odom.position.y

        bot_steer_angle = atan2(self.odom.orientation.z, self.odom.orientation.w) * 2
        goal_steer_angle = atan2(dy, dx)

        transform_angle = pi/2 - bot_steer_angle
        sin_transform_angle = sin(transform_angle)
        cos_transform_angle = cos(transform_angle)

        transformed_dx = cos_transform_angle * dx - sin_transform_angle * dy
        transformed_dy = sin_transform_angle * dx + cos_transform_angle * dy

        state = [transformed_dx, transformed_dy, self.vel.linear.x, self.vel.angular.z]
        return state, atan2(-transformed_dx, transformed_dy), dx*dx + dy*dy # state, relative steer, distance 

    def calculate_reward(self, distance, relative_steer):

        dR = sqrt(self.distance) - sqrt(distance)
        dA = -abs(relative_steer)/pi
        reward = tanh(dR*30 + dA)-1
        return reward

    def terminate_episode(self):

        self.respawn()
        self.agent.learn(self.observations, self.rewards)

        print(f"episode: {self.episode} return: {self.episode_return} epochs: {self.epochs}")

        self.episode = self.episode + 1
        self.episode_return = 0
        self.epochs = 0

        self.observations = []
        self.rewards = []

        self.agent.save(store_weights)

        self.odom = None
        self.goal = None

        self.distance = None
        self.steer = None

    def __call__(self):

        rate = rospy.Rate(10)
        while (not rospy.is_shutdown()):
            if (self.goal is None):
                self.generate_random_goal(5)
                self.distance = 5*5
                continue

            observation, relative_steer, distance = self.calculate_current_state()
            mean, std, action = self.agent.choose_action(observation)
            self.confidence_bar.scale.z = 1/std.sum().item()
            self.confidence_bar.pose.position.z = 3 + 0.5/std.sum().item()
            v, w = action
            self.vel.linear.x = v 
            self.vel.angular.z = w
            self.vel_pub.publish(self.vel)

            self.epochs += 1

            reached = bool(distance < self.goal_tolerance*self.goal_tolerance)
            reward = 1 if reached else self.calculate_reward(distance,relative_steer) 
            self.episode_return += reward
            time_up = bool(self.epochs >= 200)

            done = int(reached or time_up)

            self.observations.append(observation)
            self.rewards.append(reward)

            self.distance = distance
            self.steer = relative_steer

            if (done):
                self.terminate_episode()

            self.goal_pub.publish(self.goal_marker)
            self.confidence_pub.publish(self.confidence_bar)
            rate.sleep()

if (__name__ == "__main__"):

    trainer = TrainerNode()
    trainer()

