#! /usr/bin/python3

import os
from math import atan2, cos, sin, pi, tanh, sqrt
import random

import rospy
from nav_msgs.msg import Odometry
from geometry_msgs.msg import Twist, Point, Vector3
from visualization_msgs.msg import Marker, MarkerArray
from std_srvs.srv import Empty

from skeleton import Agent

class Trainer():

    def __init__(self):

        rospy.init_node("trainer")

        # ros init
        
        rospy.Subscriber("/odom", Odometry, self.get_odom)
        
        self.vel_pub = rospy.Publisher("/cmd_vel", Twist, queue_size=1)
        self.reward_pub = rospy.Publisher("/rewards", MarkerArray, queue_size=1)
        self.goal_pub = rospy.Publisher("/goal_mark", Marker, queue_size=1)

        self.bot_coordinates:Point = None
        self.goal_coordinates:Point = None
        self.bot_steer_angle:float = None

        self.vel = Twist()
        self.markers = MarkerArray()
        self.markers.markers = [Marker(), Marker(), Marker(), Marker()]

        for r in range(len(self.markers.markers)):
            self.markers.markers[r].header.frame_id = "odom"
            self.markers.markers[r].ns = f"reward{r}"
            self.markers.markers[r].id = r
            self.markers.markers[r].type = Marker.CYLINDER
            self.markers.markers[r].action = Marker.ADD
            self.markers.markers[r].color.r = (r == 0) + (r == 3)
            self.markers.markers[r].color.g = (r == 1) + (r == 3)
            self.markers.markers[r].color.b = (r == 2) + (r == 3)
            self.markers.markers[r].color.a = 1
            self.markers.markers[r].scale = Vector3(0.2, 0.2, 1)
            self.markers.markers[r].pose.position = Point(7, r, 0)

        self.goal_marker = Marker()
        self.goal_marker.header.frame_id = "odom"
        self.goal_marker.ns = "goal"
        self.goal_marker.id = 0
        self.goal_marker.type = Marker.SPHERE
        self.goal_marker.action = Marker.ADD
        self.goal_marker.color.r = 1
        self.goal_marker.color.g = 0
        self.goal_marker.color.b = 0
        self.goal_marker.color.a = 1
        # agent init 

        self.agent = Agent(
            observation_size=4,
            n_actions=2,
            alpha=0.001,
            beta=0.001,
            gamma=0.99,
            reuse="run_v2"
        )

        self.goal_tolerance = 0.8

        self.prev_observation:list = None
        self.prev_action:list = None
        self.prev_distance:float = None
        self.prev_steer:float = None

        self.observations = []
        self.next_observations = []
        self.rewards = []
        self.actions = []
        self.terminals = []

        self.episode = 1
        self.epochs = 1
        self.episode_return = 0

    def respawn(self):
        try:
            reset = rospy.ServiceProxy("/gazebo/reset_simulation", Empty)
            reset()
        except rospy.ServiceException as e:
            print(f"{type(e)}: {e}")


    def get_odom(self, odom):
        self.bot_coordinates = odom.pose.pose.position
        self.bot_steer_angle = atan2(odom.pose.pose.orientation.z, odom.pose.pose.orientation.w) * 2

    def generate_random_goal(self, distance):
        if (self.bot_coordinates is None):
            return None
        theta = random.random() * 2 * pi 
        dx = distance * cos(theta)
        dy = distance * sin(theta)

        self.goal_coordinates = Point(dx, dy, 0)
        
        self.goal_marker.pose.position = Point(dx, dy, 0)
        self.goal_marker.scale = Vector3(*([self.goal_tolerance]*3))

    def reached(self, distance):
        if (self.goal_coordinates is None):
            return False
        return distance < self.goal_tolerance*self.goal_tolerance

    def reward_function(self, observation, distance, steer):
        dv = self.vel.linear.x - observation[2]
        dw = self.vel.angular.z - observation[3]

        del_distance = sqrt(distance) - sqrt(self.prev_distance)
        del_steer = abs(steer) - abs(self.prev_steer)

        reward1 = 0.02*(dv*dv + dw*dw) # leftward pushing (-ve)
        reward2 = 4*(del_distance) # leftward pushing (-ve)
        reward3 = 2*(del_steer) # leftward pushing (-ve)
        
        rewards = [reward1, reward2, reward3]
        for r in range(len(rewards)):
            self.markers.markers[r].pose.position.z = rewards[r]/2
            self.markers.markers[r].scale.z = rewards[r]

        reward = reward1 + reward2 + reward3
        reward = tanh(-reward) - 1

        self.markers.markers[3].pose.position.z = reward/4
        self.markers.markers[3].scale.z = reward/2

        return reward/2

    def __call__(self):

        rate = rospy.Rate(10)
        while (not rospy.is_shutdown()):
            if (self.goal_coordinates is None):
                self.generate_random_goal(5)
                continue
        
            self.reward_pub.publish(self.markers)
            self.goal_pub.publish(self.goal_marker)

            dx = self.goal_coordinates.x - self.bot_coordinates.x
            dy = self.goal_coordinates.y - self.bot_coordinates.y
            distance = dx*dx + dy*dy

            del_angle = pi/2 - self.bot_steer_angle
            ndx, ndy = dx*cos(del_angle) - dy*sin(del_angle), dx*sin(del_angle) + dy*cos(del_angle)

            steer = atan2(ndy, ndx)
            observation = [ndx, ndy, self.vel.linear.x, self.vel.angular.z]
            v, w = self.agent.choose_action(observation)

            self.vel.linear.x = v.item()
            self.vel.angular.z = w.item()
            self.vel_pub.publish(self.vel)

            if (self.prev_action is None or 
                self.prev_distance is None or
                self.prev_observation is None or
                self.prev_steer is None):
                self.prev_action = [v, w]
                self.prev_distance = distance
                self.prev_observation = observation
                self.prev_steer = steer
                continue

            self.epochs += 1

            reached = self.reached(distance)
            time_up = self.epochs > 600
            done = int(reached or time_up)

            reward = self.reward_function(observation, distance, steer)
            self.episode_return += reward

            self.observations.append(self.prev_observation)
            self.next_observations.append(observation)
            self.actions.append(self.prev_action)
            self.rewards.append([reward])
            self.terminals.append([done])

            self.prev_action = [v, w]
            self.prev_distance = distance
            self.prev_observation = observation
            self.prev_steer = steer

            if (done):
                # respawn
                self.respawn()
                print(f"episode: {self.episode} return: {self.episode_return} epochs: {self.epochs}")
                
                self.epochs = 0
                self.episode += 1
                self.episode_return = 0

                self.agent.backprop(
                    self.observations,
                    self.next_observations,
                    self.actions,
                    self.rewards,
                    self.terminals)

                self.agent.save(os.sys.argv[1])

                self.observations = []
                self.next_observations = []
                self.actions = []
                self.rewards = []
                self.terminals = []

                self.prev_action = None
                self.prev_distance = None
                self.prev_observation = None
                self.prev_steer = None

                self.vel = Twist()

                self.goal_coordinates = None
                self.bot_coordinates = None

            rate.sleep()

if __name__ == "__main__":

    trainer = Trainer()
    trainer()

